---
title: "New_Trial"
output:
  html_document:
    df_print: paged
---
# Front matter
```{r, echo=FALSE}
#Packages
# always clean up R environment
rm(list = ls())
library(dplyr)
library(tidyverse)
library(mosaic)
library(reticulate)
library(data.table)
library(ggplot2)
library(DAAG)
library(bootstrap)
library(rpart)
library(Metrics)
library(readr)
library(memisc)
library(plotly)
library(neuralnet)
library(randomForest)
library(VIM)
library(pvclust)
library(mclust)
library(cluster)

#data
kaggle <- read.csv("kaggle_edu.csv", header = TRUE)
```

Kaggle Data Set Contains 17 Features
```{r}
names(kaggle)
```

The overarching goal is to quantify a students success based on their participation in school. We define success based on a studetns ability to have High Parent School Satisfaction Rates & Receiving Medium-High Grades. We define participation as: Accessing School Resources, Participating in Class (Raising Hands & Answering Questions) and Not Being Absent too many times. 
The following graphs show the realtionships between different variables in our dataset. 

Grades are separated from Low(0-69) Middle (70,89) and High (90-100)

```{r}
#Raising Hands 
#Are student using their resources provided??? And how does it affect their grades????
#Are student that use resources often times better prepared

#box plot 
#Comparre Raising Hands with Gender
ggplot(data = kaggle, aes(x = gender, y = raisedhands)) + geom_boxplot()
#Resources With Gender
ggplot(data = kaggle, aes(x = gender, y = VisITedResources)) + geom_boxplot()

#Higher Grades Associated with Students that raise hands more
ggplot(data = kaggle, aes(x = Class, y = raisedhands)) + geom_boxplot()
#Higher Grades Associated with Students that visit more resources
ggplot(data = kaggle, aes(x = Class, y = VisITedResources)) + geom_boxplot()

#With mum as the relation more students paticipate
ggplot(data = kaggle, aes(x = Relation, y = raisedhands)) + geom_boxplot()


#Students with better satisfaction raise their hands mroe
ggplot(data = kaggle, aes(x = ParentschoolSatisfaction, y = raisedhands)) + geom_boxplot()
#Students with better satisfaction use more resources
ggplot(data = kaggle, aes(x = ParentschoolSatisfaction, y = VisITedResources)) + geom_boxplot()


#density plots
#Raising Hands Based on Topic
ggplot(data = kaggle, aes(x = raisedhands, color = Topic)) + geom_density()
```
I find it interesting how there is such a disparity between female and male participation and how female figureheads on average lead to more particiaption among students in the classroom. I find evidence to suggest that there is a positve relationship between a students success in school and their ability to participate in class. Our next steps will be do quantify this relationship using different models such as simple linear regressoin, multi-linear regression, neural networks & k-means clustering. 


```{r}
#Simple Linear Regression
#Statistical Modeling 
#Middle = 3 | Low = 2 | High = 1 
kaggle1 <- kaggle %>%
  mutate(Class = as.numeric(kaggle$Class))

model <- lm(VisITedResources ~ raisedhands, data = kaggle1)
model1 = summary(model)

kaggle1$predicted <- predict(model)   # Save the predicted values
kaggle1$residuals <- residuals(model)

kaggle1 %>% select(VisITedResources, predicted, residuals) %>% head()

ggplot(kaggle1, aes(x = raisedhands, y = VisITedResources)) +  
  geom_point()


#Connecting actaul data points with pred values
ggplot(kaggle1, aes(x = raisedhands, y = VisITedResources)) +
  geom_segment(aes(xend = raisedhands, yend = predicted)) +
  geom_point() +
  geom_point(aes(y = predicted), shape = 1)

#Clean up 
ggplot(kaggle1, aes(x = raisedhands, y = VisITedResources))  +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +  # Plot regression slope
  geom_segment(aes(xend = raisedhands, yend = predicted), alpha = .2) +  # alpha to fade lines
  geom_point() +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw() 

#Use residuals to adjust 
ggplot(kaggle1, aes(x = raisedhands, y = VisITedResources))+
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = raisedhands, yend = predicted), alpha = .2) +

  # > Alpha adjustments made here...
  geom_point(aes(alpha = abs(residuals))) +  # Alpha mapped to abs(residuals)
  guides(alpha = FALSE) +  # Alpha legend removed
  # <

  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()


# COLOR
# High residuals (in abolsute terms) made more red on actual values.
ggplot(kaggle1, aes(x = raisedhands, y = VisITedResources)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = raisedhands, yend = predicted), alpha = .2) +

  # > Color adjustments made here...
  geom_point(aes(color = abs(residuals))) + # Color mapped to abs(residuals)
  scale_color_continuous(low = "black", high = "red") +  # Colors to use here
  guides(color = FALSE) +  # Color legend removed
  # <

  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()


ggplot(kaggle1, aes(x = raisedhands, y = VisITedResources)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = raisedhands, yend = predicted), alpha = .2) +

  # > Color AND size adjustments made here...
  geom_point(aes(color = abs(residuals), size = abs(residuals))) + # size also mapped
  scale_color_continuous(low = "black", high = "red") +
  guides(color = FALSE, size = FALSE) +  # Size legend also removed
  # <

  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
```

58.6 percent of variation in the class grade is explained by the multiple linear regression with the predictors.
We looked at four factors to determine if the predictors were fit for multiple linear regression. The graphs will explain, linearity, independence, normality and equal variance among residuals. The first graph shows the linearity of the predictors, with some outliers. The QQ plot shows that the predictors are normal. The scale-location plot is showing the equal variance among residuals, and the spread is equal across the graph, making them have equal variance. The residuals vs leverage plot helps us find influential cases if any at all, which they are all around the cooks distance line, so no influential cases. To check for collinearity in the predictors, the variance inflation factor was used. These numbers showed below 10 therefore no predictors have collinearity.
```{r}
#Multiple Linear Regression
d <- kaggle1 %>%
  select(VisITedResources, raisedhands, Class, PlaceofBirth, StudentAbsenceDays, Topic, Relation, gender)

fit <- lm(d$VisITedResources ~  ., data = d)
summary(fit)
#R-Squared is 57.9%
d$predicted <- predict(fit)
d$residuals <- residuals(fit)

#Significant Values: Raised Hands, Place of Birth- Libya, Absent Days under 7, French Topic, French & MAth, Gender, RelationMum 

#Comparing Actual Vs predicted....
plot(predict(fit),d$VisITedResources,
      xlab="predicted",ylab="actual")
abline(a=0,b=1)

par(mfrow = c(2, 2))
plot(fit)

#Use of of Gather Key 
d %>% 
  gather(key = "iv", value = "x", -VisITedResources, -predicted, -residuals) %>%  # Get data into shape
  ggplot(aes(x = x, y = VisITedResources)) +  # Note use of `x` here and next line
  geom_segment(aes(xend = x, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  facet_grid(~ iv, scales = "free_x") +  # Split panels here by `iv`
  theme_bw()
```

We use an Artificial Neural Netwrok with three hidden layers. To properly use the model we need to convert all of our factor variables into numerical variables so we beign with some data wrangling and mutate all factor variables. We then normalize the data so we can use it properly and we created a user defined function that normalizes our data. We then split the data into training/testing data (75-25 split). I give a visualization of the NN and make predictions of how likely a student is to utilize school resources and from this data set it is about 60% of student utilize their resources. The model has an accuracy of 79.85%. 

```{r}
#neural network
#Neural Network tools for Big Data  
#Father = 1 | Mother = 2
d <- kaggle1 %>%
  select(VisITedResources, raisedhands, Class, PlaceofBirth, StudentAbsenceDays, Topic, Relation, gender)

neural_df <- d %>%
  mutate(Relation = as.numeric(d$Relation)) %>%
  mutate(PlaceofBirth = as.numeric(d$PlaceofBirth)) %>%
  mutate(Topic = as.numeric(d$Topic)) %>% 
  mutate(gender = as.numeric(d$gender)) %>%
  mutate(StudentAbsenceDays = as.numeric(d$StudentAbsenceDays))

#Normalize Data
norm.fun = function(x){ 
  (x - min(x))/(max(x) - min(x)) 
}
neural_df.norm = apply(neural_df, 2, norm.fun)

#training set- on normalized data 
set.seed(7736)
len <- nrow(neural_df.norm)
testindices <- sample(len, round(len/4))
train_data <- neural_df.norm[-testindices,]
test_data <- neural_df.norm[testindices,]

#Fit NN on normalized training dat
nn=neuralnet(VisITedResources ~ raisedhands + PlaceofBirth + StudentAbsenceDays + Topic + Relation + gender, data = train_data , hidden=3,act.fct = "logistic", linear.output = FALSE)

plot(nn)

#predict test results
Predict=compute(nn,test_data)
Predict$net.result

# Converting probabilities into binary classes setting threshold level 0.5
#1 being used more likely to use resources, 0 being not
prob <- Predict$net.result
pred <- ifelse(prob>0.5, 1, 0)
pred
sum(pred == 1)
cat("The proportion of students that were likely to take advantage of resources are:", round(sum(pred == 1)/nrow(pred) *100,2),"%")

#Show predictions being made
plot(pred,test_data[,1],
      xlab="predicted",ylab="actual")
abline(a=0,b=1, col = "blue")
abline(h=0.5, col = 'red')

test_data$pred <- pred

#Compare to grade received in class
used_resour<- which(pred == 1)
test_data <- unlist(test_data)
resc_df <- test_data[used_resour]
new_df <- as.data.frame(resc_df)
mean(resc_df)
```


```{r}
# Create a Random Forest model with default parameters
#General Purpose Data Wrangling 
RF_df.norm <- d %>%
  mutate(Relation = as.numeric(d$Relation)) %>%
  mutate(PlaceofBirth = as.numeric(d$PlaceofBirth)) %>%
  mutate(Topic = as.numeric(d$Topic)) %>% 
  mutate(gender = as.numeric(d$gender)) %>%
  mutate(StudentAbsenceDays = as.numeric(d$StudentAbsenceDays))
#Normlaize Daya
#User Defined Function
norm.fun = function(x){ 
  (x - min(x))/(max(x) - min(x)) 
}

RF_df.norm = apply(RF_df.norm, 2, norm.fun)

#Train/Test Data
set.seed(7736)
len <- nrow(RF_df.norm)
testindices <- sample(len, round(len/4))
train_data <- RF_df.norm[-testindices,]
test_data <- RF_df.norm[testindices,]

#Random Forest
rand_forest<- randomForest(VisITedResources ~ ., data = train_data, importance = TRUE)

rand_forest

# Fine tuning parameters of Random Forest model
rand_forst2 <- randomForest(VisITedResources ~ ., data = train_data, ntree = 500, mtry = 6, importance = TRUE)
rand_forst2

# Predicting on train set
predTrain <- predict(rand_forst2, train_data, type = "class")
# Checking classification accuracy
#table(predTrain, train_data[,1]) 

predValid <- predict(rand_forst2, test_data, type = "class")
mean(predValid == test_data[1,])                    

```

Here we use an unsupervised approach of K-means clustering. We cluster based on students that Visit Resources into 3 different clusters.
By using the k means elbow method we determine that we should use a K value of 4, so we have 4 clusters.
```{r}
#Unsupervised Clustering Algo 
#K-means clustering 
# 'aggr' plots the amount of missing/imputed values in each column
aggr(d)

set.seed(20)
clusters <- kmeans(d[,1:3], 3)

clusters$cluster <- as.factor(clusters$cluster)
ggplot(d, aes(VisITedResources, raisedhands, color = clusters$cluster)) + geom_point()

new_d <- d %>%
  mutate(StudentAbsenceDays = as.numeric(d$StudentAbsenceDays)) %>%
  select(VisITedResources, raisedhands, StudentAbsenceDays, Class)

mydata <- scale(new_d) # standardize variables
# Determine number of clusters
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
#for loop 
#Elbow Method 
for (i in 2:15) wss[i] <- sum(kmeans(mydata, 
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")


# K-Means Cluster Analysis
fit <- kmeans(mydata, 4) # 4 cluster solution
# get cluster means 
aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata <- data.frame(mydata, fit$cluster)


# Ward Hierarchical Clustering
dat <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(dat, method="ward") 
plot(fit) # display dendogram
groups <- cutree(fit, k=4) # cut tree into 4 clusters
# draw dendogram with red borders around the 4 clusters 
rect.hclust(fit, k=4, border="red")

fit <- pvclust(mydata, method.hclust="ward",
   method.dist="euclidean")
plot(fit) # dendogram with p values
# add rectangles around groups highly supported by the data
pvrect(fit, alpha=.95)


# K-Means Clustering with 4 clusters
fit <- kmeans(mydata, 4)

# Cluster Plot against 1st 2 principal components

# vary parameters for most readable graph
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, 
   labels=2, lines=0)
```

##Conclusion

I am confident that from my findings it is important to stress to students that availability of resourcess at their disposal. It is apparent that there is some kind of correlation between being an involved student and succeeding in school. Of course this does not apply to all students and should be taken with a grain of salt; nonetheless, I feel that if more studetns were to be involved the university would see an uptick in average GPA across the board. My next step was to use Penn State Students Degree Audits to cross-reference it with my models to provide reccomendations to get more involved and based on these reccomendations offer a prediction interval for their forecasted new GPA. I faced numerous problems with using DegreeAudit Data like ethical concerns, but mostly the fact it was a heavily datapoor set and my accuracy scores were in the range of 10%:20%. I think it would be very useful to offer students a tool that would show them how better they could potentially perform if they were to utilize these resources. Resources would include Office Hours, Campus Tutoring, Meeting with TA’s. 


